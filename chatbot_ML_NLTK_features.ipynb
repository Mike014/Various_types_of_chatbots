{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK: Introduction and Functions\n",
    "## What is NLTK? **NLTK** is a Python library used for natural language processing **(NLP)** tasks, such as developing chatbots. It is particularly useful for analyzing and understanding texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Operations of NLTK\n",
    "\n",
    "1. Tokenization: Splits a paragraph or sentence into smaller units, such as words or sentences.\n",
    "2. Stop Words Removal: Removes insignificant words (e.g., articles, conjunctions) that do not add semantic value.\n",
    "3. Stemming: Reduces a word to its root form by removing endings or suffixes.\n",
    "4. Lemmatization: Groups the variants of a word into its root form.\n",
    "5. Chunking: Extracts meaningful phrases or groups from unstructured data.\n",
    "6. Chinking: Removes parts of a sentence or group identified by chunking.\n",
    "7. Named Entity Recognition (NER): Identifies and classifies entities in a text such as proper names, places, or organizations.\n",
    "8. Part of Speech Tagging (POS Tagging): Classifies each word based on its grammatical function (noun, verb, adjective, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dell\\anaconda3\\envs\\my_env\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\envs\\my_env\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\envs\\my_env\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\anaconda3\\envs\\my_env\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\envs\\my_env\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\envs\\my_env\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # For tokenization\n",
    "nltk.download('punkt_tab') # For tokenization\n",
    "nltk.download('stopwords')  # For stopwords\n",
    "nltk.download('wordnet')  # For lemmatization\n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
    "nltk.download('maxent_ne_chunker')  # For NER\n",
    "nltk.download('words')  # For NER\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'is', 'a', 'great', 'library', 'for', 'natural', 'language', 'processing', '.', 'It', 'makes', 'text', 'analysis', 'simple', '.']\n",
      "['NLTK is a great library for natural language processing.', 'It makes text analysis simple.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Example text\n",
    "text = \"NLTK is a great library for natural language processing. It makes text analysis simple.\"\n",
    "\n",
    "# Word tokenization\n",
    "print(word_tokenize(text))\n",
    "\n",
    "# Sentence tokenization\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'great', 'library', 'natural', 'language', 'processing', '.', 'makes', 'text', 'analysis', 'simple', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get the English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# List of words\n",
    "filtered_words = [word for word in word_tokenize(text) if word.lower() not in stop_words]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nltk', 'great', 'librari', 'natur', 'languag', 'process', '.', 'make', 'text', 'analysi', 'simpl', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'great', 'library', 'natural', 'language', 'processing', '.', 'make', 'text', 'analysis', 'simple', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.'), ('It', 'PRP'), ('makes', 'VBZ'), ('text', 'JJ'), ('analysis', 'NN'), ('simple', 'NN'), ('.', '.')]\n",
      "Frase chunked: (S\n",
      "  NLTK/NNP\n",
      "  is/VBZ\n",
      "  (NP a/DT great/JJ library/NN)\n",
      "  for/IN\n",
      "  (NP natural/JJ language/NN)\n",
      "  (NP processing/NN)\n",
      "  ./.\n",
      "  It/PRP\n",
      "  makes/VBZ\n",
      "  (NP text/JJ analysis/NN)\n",
      "  (NP simple/NN)\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Chunking\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "words_with_pos = pos_tag(word_tokenize(text))\n",
    "print(words_with_pos)\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"  # Nominal Group Pronouns\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "chunked = chunk_parser.parse(words_with_pos)\n",
    "print(\"Frase chunked:\", chunked)\n",
    "chunked.draw()  # Show the diagram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (ORGANIZATION NLTK/NNP)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  great/JJ\n",
      "  library/NN\n",
      "  for/IN\n",
      "  natural/JJ\n",
      "  language/NN\n",
      "  processing/NN\n",
      "  ./.\n",
      "  It/PRP\n",
      "  makes/VBZ\n",
      "  text/JJ\n",
      "  analysis/NN\n",
      "  simple/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "\n",
    "entities = ne_chunk(words_with_pos)\n",
    "print(entities)\n",
    "\n",
    "entities.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "words_with_pos = pos_tag(word_tokenize(text))\n",
    "\n",
    "print(words_with_pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
